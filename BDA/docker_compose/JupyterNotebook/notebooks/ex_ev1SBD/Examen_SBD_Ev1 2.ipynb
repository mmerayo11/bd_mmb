{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7950a9f9-ebef-48f1-b7ef-74689ce05ae5",
   "metadata": {},
   "source": [
    "# SISTEMAS DE BIG DATA - Examen 1ª Evaluación\n",
    "\n",
    "**Instrucciones generales**\n",
    "\n",
    "1.\tTodas las sentencias deben ejecutarse desde la línea de comandos en las celdas que hay después del enunciado. No debes realizar ninguna tarea desde fuera de Jupyter.\n",
    "2.\tPuedes **añadir** todas las celdas que necesites siempre y cuando estén antes del siguiente enunciado.\n",
    "3.\tTodas las celdas **deben estar ejecutadas** y debe visualizarse el resultado de salida.\n",
    "4.\t**No es necesario documentar** las respuestas, simplemente debes hacer lo que se pide en el enunciado.\n",
    "5.\tDespués de cada parte debes insertar una **captura de pantalla** del cliente gráfico de la base de datos correspondientes donde se vea que los datos se han cargado correctamente.\n",
    "6.\tDebes entregar tanto el **notebook** (fichero `.ipynb`) como el mismo fichero convertido a **PDF** (es muy probable que si intentas convertirlo en el propio contenedor te falle por no tener instalado `pandoc`, si es así descargalo en formato `.md` o `html` y conviértelo en tu máquina física)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ee366-1158-4061-a6f1-ec4c6fde2148",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**NOMBRE**:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efdd2495-4e56-4100-9538-51e8545144af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: influxdb-client in /opt/conda/lib/python3.11/site-packages (1.49.0)\n",
      "Requirement already satisfied: reactivex>=4.0.4 in /opt/conda/lib/python3.11/site-packages (from influxdb-client) (4.1.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.11/site-packages (from influxdb-client) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.11/site-packages (from influxdb-client) (2.8.2)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.11/site-packages (from influxdb-client) (68.2.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from influxdb-client) (2.0.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.5.3->influxdb-client) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.1.1 in /opt/conda/lib/python3.11/site-packages (from reactivex>=4.0.4->influxdb-client) (4.8.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (5.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install influxdb-client\n",
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d393dc33-a532-4bba-b10a-28d4f9e76d2e",
   "metadata": {},
   "source": [
    "## Contexto del escenario\n",
    "\n",
    "Has sido contratado por una fábrica inteligente que dispone de sensores de temperatura y vibración en sus máquinas críticas. La empresa necesita un sistema backend capaz de procesar los datos que llegan de los sensores en tiempo real.\n",
    "\n",
    "El sistema debe cumplir dos objetivos simultáneos:\n",
    "\n",
    "1.  **Monitorización en vivo (Dashboard):** los operarios necesitan saber el estado *actual* de cada máquina y si hay alguna alarma activa en este preciso instante. Para esto usarás **Redis**.\n",
    "2.  **Histórico para mantenimiento predictivo:** el equipo de Data Science necesita almacenar todos los datos brutos a lo largo del tiempo para entrenar modelos de IA futuros. Para esto usarás **InfluxDB**.\n",
    "\n",
    "## Los Datos de Entrada\n",
    "\n",
    "Los datos con los que vas a trabajar los tienes en el *dataset* sintético adjunto llamado `sensores.csv`. Este *dataset* contiene lecturas simuladas con las siguientes columnas:\n",
    "\n",
    "  - `timestamp`: fecha y hora del evento.\n",
    "  - `machine_id`: identificador único de la máquina.\n",
    "  - `zone`: zona de la fábrica.\n",
    "  - `temperature`: temperatura en grados Celsius.\n",
    "  - `vibration`: nivel de vibración (0-100).\n",
    "  - `lat`, `lon`: coordenadas del robot.\n",
    "  - `status`: estado reportado por la máquina (\"OK\", \"WARNING\", \"ERROR\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e1bde7-5156-4b81-a5c3-4e903f190a0c",
   "metadata": {},
   "source": [
    "**IMPORTANTE**\n",
    "\n",
    "El desarrollo del examen debe de ser modular, con un programa principal que inicialice las conexiones a la base de datos y lea los datos del fichero y luego invocará **una función diferente para cargar cada tipo de dato** en la base de datos\n",
    "\n",
    "Es decision tuya elegir los parámetros que recibirá cada función, aunque es altamente aconsejable **no utilizar variables globales**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c1c3d1-5976-44e0-8f12-b0e36ed809cd",
   "metadata": {},
   "source": [
    "## Parte A: Persistencia histórica (InfluxDB)\n",
    "\n",
    "`2 puntos`\n",
    "\n",
    "En esta parte tienes que crear un script que lea el fichero CSV facilitado y almacene los datos en una base de datos InfluxDB.\n",
    "\n",
    "Los aspectos que tienes que tener en cuenta son:\n",
    "\n",
    "  - **Bucket:** `factory_logs`\n",
    "  - **Measurement:** `maquinaria`\n",
    "  - **Requisito clave:** debes modelar correctamente los datos usando adecuadamente *tags* o *fields* según el tipo de datos. Se debe respetar el `timestamp` del datos (no usar el tiempo de ingesta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afc05b75-32a2-444d-9ff5-5463c45a1142",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2050943277.py, line 82)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 82\u001b[0;36m\u001b[0m\n\u001b[0;31m    except NewConnectionError:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import time\n",
    "import csv\n",
    "import sys\n",
    "import influxdb_client\n",
    "from datetime import datetime\n",
    "from influxdb_client import Point, InfluxDBClient\n",
    "from influxdb_client.client.write_api import WriteOptions, WriteType, ASYNCHRONOUS\n",
    "from influxdb_client.client.exceptions import InfluxDBError\n",
    "from urllib3.exceptions import NewConnectionError\n",
    "\n",
    "puntos_fichero = 0\n",
    "TIME_FORMAT = '%Y-%m-%d %H:%M:%S'\n",
    "\n",
    "INFLUX_URL = \"http://influxdb2:8086\"\n",
    "INFLUX_TOKEN = \"pY8NneFC5iDvPM2LKuL1v-e1NR7was3ddvW5g6jniZ30voUqkoRvEIg74VBwioQowBIGHh-4FjPy60lW24e1vQ==\"\n",
    "INFLUX_ORG = \"docs\"\n",
    "BUCKET_NAME = \"factory_logs\"\n",
    "FILE_NAME = \"telemetria_agv.csv\"\n",
    "\n",
    "MEASUREMENT_NAME = \"maquinaria\"\n",
    "\n",
    "client = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)\n",
    "\n",
    "def success_callback(conf, data):\n",
    "    \"\"\"Callback al finalizar un batch con éxito.\"\"\"\n",
    "    pass \n",
    "\n",
    "def error_callback(conf, data, exception):\n",
    "    \"\"\"Callback para manejar errores de escritura.\"\"\"\n",
    "    print(f\"\\n--- ERROR DE ESCRITURA ---\")\n",
    "    print(f\"Excepción: {exception}\")\n",
    "    print(f\"--------------------------\\n\")\n",
    "\n",
    "write_api = client.write_api(\n",
    "        write_options=ASYNCHRONOUS,\n",
    "        success_callback=success_callback,\n",
    "        error_callback=error_callback\n",
    "    )\n",
    "\n",
    "with open(FILE_NAME, mode='r', newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "\n",
    "    next(reader, None)\n",
    "\n",
    "    for row in reader:\n",
    "        if len(row) < 8:\n",
    "            continue\n",
    "    \n",
    "        timestamp_str = row[0]\n",
    "        timestamp_dt = datetime.strptime(timestamp_str, TIME_FORMAT)\n",
    "        \n",
    "        machine_id = row[1]\n",
    "        zone = row[2]\n",
    "        temperature = float(row[3])\n",
    "        vibration = float(row[4])\n",
    "        lat = float(row[5])\n",
    "        lon = float(row[6])\n",
    "        status = row[7]\n",
    "    \n",
    "        point = (\n",
    "            Point(MEASUREMENT_NAME)\n",
    "            .tag(\"machine_id\", machine_id)\n",
    "            .time(timestamp_dt) \n",
    "            .field(\"zone\", zone)\n",
    "            .field(\"temperature\", temperature)\n",
    "            .field(\"vibration\", vibration)\n",
    "            .field(\"lat\", lat)\n",
    "            .field(\"lon\", lon)\n",
    "            .field(\"status\", status)\n",
    "        )\n",
    "        \n",
    "        write_api.write(bucket=BUCKET_NAME, record=point)\n",
    "    \n",
    "        puntos_fichero += 1\n",
    "                        \n",
    "        print(f\"   -> {puntos_fichero} puntos procesados\", end='\\r')\n",
    "\n",
    "\n",
    "    print(f\"\\nProcesamiento de {puntos_fichero} puntos finalizado.\")\n",
    "\n",
    "# Asegúrate de cerrar la API para vaciar el buffer\n",
    "write_api.close()\n",
    "client.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ed5349-3dab-4524-8061-ef99218adcbc",
   "metadata": {},
   "source": [
    "## Parte B - Analítica en tiempo real con Redis\n",
    "\n",
    "Debes crear un script que alimente las siguientes estructuras en Redis por cada dato procesado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "89dfc441-1d2b-473b-ab6e-badcd999b118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: redis in /opt/conda/lib/python3.11/site-packages (6.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc31d35-9b8d-4c0e-9a3f-8f89f437ddcd",
   "metadata": {},
   "source": [
    "### 1.- Estadísticas agregadas\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "Al procesar masivamente datos de telemetría, es costoso consultar la base de datos histórica (InfluxDB) para preguntas simples como \"¿Cuál ha sido la temperatura máxima hoy en el Almacén A?\". Vamos a usar Redis Hashes para mantener un marcador actualizado de estadísticas por zona.\n",
    "\n",
    "Para cada fila procesada del CSV, debes actualizar un Hash correspondiente a la Zona (zone) donde se encuentra el robot.\n",
    "\n",
    "- **Clave:** `stats:zone:{nombre_zona}` (Ej: stats:zone:Almacen_A, stats:zone:Recepcion...).\n",
    "- **Campos:**:\n",
    "    - `total_lecturas`: contador total de datos recibidos de esa zona.\n",
    "    - `total_errores`: contador de cuántas veces el status ha sido \"ERROR\".\n",
    "    - `max_temp`: La temperatura más alta registrada hasta el momento en esa zona."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "422b5288-dcf7-4cd9-a886-d44f639ac796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import redis\n",
    "\n",
    "r = redis.Redis(\n",
    "    host='redis',\n",
    "    port=6379,\n",
    "    db=0,\n",
    "    decode_responses=True\n",
    ")\n",
    "print(r.ping())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d10a2611-5cf4-44a0-8e74-dc1136560ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zona 'stats:zone:Almacen_A' añadido.\n",
      "Zona 'stats:zone:Almacen_B' añadido.\n",
      "Zona 'stats:zone:Recepcion' añadido.\n",
      "Zona 'stats:zone:Ensamblaje' añadido.\n",
      "Zona 'stats:zone:Expediciones' añadido.\n"
     ]
    }
   ],
   "source": [
    "reader = csv.reader(sys.stdin)\n",
    "\n",
    "max_tempA = 0\n",
    "max_tempB = 0\n",
    "max_tempR = 0\n",
    "max_tempEn = 0\n",
    "max_tempEx = 0\n",
    "\n",
    "lecA = 0\n",
    "lecB = 0\n",
    "lecR = 0\n",
    "lecEn = 0\n",
    "lecEx = 0\n",
    "\n",
    "errA = 0\n",
    "errB = 0\n",
    "errR = 0\n",
    "errEn = 0\n",
    "errEx = 0\n",
    "\n",
    "for row in reader:\n",
    "\n",
    "    if row[1] == \"machine_id\":\n",
    "        continue\n",
    "    try:\n",
    "        zone = row[2]\n",
    "        status = row[7]\n",
    "        temp = row[3]\n",
    "    except ValueError:\n",
    "         continue\n",
    "\n",
    "    if zone == \"Almacen_A\" :\n",
    "        lecA += 1\n",
    "        if status == \"ERROR\":\n",
    "            errA +=1\n",
    "        if temp> max_tempA:\n",
    "            max_tempA = temp\n",
    "\n",
    "    if zone == \"Almacen_B\" :\n",
    "        lecB += 1\n",
    "        if status == \"ERROR\":\n",
    "            errB +=1\n",
    "        if temp> max_tempB:\n",
    "            max_tempB = temp\n",
    "\n",
    "    if zone == \"Recepcion\" :\n",
    "        lecR += 1\n",
    "        if status == \"ERROR\":\n",
    "            errR +=1\n",
    "        if temp> max_tempR:\n",
    "            max_tempR = temp\n",
    "\n",
    "    if zone == \"Ensamblaje\" :\n",
    "        lecEn += 1\n",
    "        if status == \"ERROR\":\n",
    "            errEn +=1\n",
    "        if temp> max_tempEn:\n",
    "            max_tempEn = temp\n",
    "\n",
    "    if zone == \"Expediciones\" :\n",
    "        lecEx += 1\n",
    "        if status == \"ERROR\":\n",
    "            errEx +=1\n",
    "        if temp> max_tempEx:\n",
    "            max_tempEx = temp\n",
    "\n",
    "def add_zone(zone, total_lecturas, total_errores, max_temp):\n",
    "    clave_zona = f\"stats:zone:{zone}\"\n",
    "    \n",
    "    Hzone= {\n",
    "        \"lecturas\" : total_lecturas,\n",
    "        \"errores\" : total_errores,\n",
    "        \"max_temp\" : max_temp\n",
    "    }\n",
    "\n",
    "    r.hset(clave_zona, mapping=Hzone)\n",
    "    \n",
    "    print(f\"Zona '{clave_zona}' añadido.\")\n",
    "\n",
    "add_zone(\"Almacen_A\", lecA, errA, max_tempA)\n",
    "add_zone(\"Almacen_B\", lecB, errB, max_tempB)\n",
    "add_zone(\"Recepcion\", lecR, errR, max_tempR)\n",
    "add_zone(\"Ensamblaje\", lecEn, errEn, max_tempEn)\n",
    "add_zone(\"Expediciones\", lecEx, errEx, max_tempEx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f03f286-7c4a-4e9e-bb24-0d10e4a3107e",
   "metadata": {},
   "source": [
    "### 2.- Ranking de \"puntos calientes\" (Sorted Set)\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "El jefe de planta quiere ver en una pantalla un \"Top de Máquinas con mayor temperatura\" ordenado de mayor a menor en tiempo real.\n",
    "\n",
    "- **Estructura:** `Sorted Set` (ZSET)\n",
    "- **Clave:** `dashboard:hottest_machines`\n",
    "- **Score:** La temperatura actual (`temperature`).\n",
    "- **Member:** El ID de la máquina (`machine_id`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee87d30-521d-4635-8c97-32abf50680c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que carga el sorted set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ddfd7b-b7a5-4c34-b9ef-40cae6419d07",
   "metadata": {},
   "source": [
    "### 3.- Seguimiento de flota (Geospatial)\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "Las máquinas de este escenario son AGVs (robots móviles) que se mueven por la planta. Necesitamos saber su ubicación exacta.\n",
    "\n",
    "- **Estructura:** `Geo`\n",
    "- **Clave:** `factory:map`\n",
    "- **Datos:** Usa la latitud y longitud que vienen en el CSV para posicionar el `machine_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ecc26b-134c-4f0e-b72e-742c2059128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que carga los datos geoespaciales\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999afa47-8719-4cd0-9c26-921de0349f5b",
   "metadata": {},
   "source": [
    "### 4.- Contadores globales atómicos (String)\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "Necesitamos estadísticas rápidas que no requieran contar filas en una base de datos histórica.\n",
    "\n",
    "- **Estructura:** `String` (Contador)\n",
    "- **Clave:** `stats:total_processed` -\\> Incrementar en 1 por cada fila procesada.\n",
    "- **Clave:** `stats:total_errors` -\\> Incrementar en 1 solo si el `status` es \"ERROR\".\n",
    "- **Clave:** `stats:total_warnings` -\\> Incrementar en 1 solo si el `status` es \"WARNING\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47fc3d1-cd1f-4e77-8226-0e0bd571fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que gestiona los contadores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193de2fc-b8ed-4bfc-9bb6-a7c8e6aec991",
   "metadata": {},
   "source": [
    "### 5.- Cola de anomalías críticas (List)\n",
    "\n",
    "`1 punto`\n",
    "\n",
    "Queremos tener también una cola de anomalías críticas. Por cada registro cuyo `status` sea `ERROR` deberás crear un JSON y almacenarlo en una estructura tipo FIFO:\n",
    "\n",
    "- **Estructura:** `List`\n",
    "- **Clave:** `alerts:queue`\n",
    "- **Datos:**: el JSON debe incluir: `machine_id`, `timestamp` y un mensaje: *\"Critical failure at [Lat, Lon]\"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81917494-a1c2-4dba-b4fe-64d6595b3f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que carga los datos en la cola\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df4799-0663-4c9e-bb15-590e9c97441b",
   "metadata": {},
   "source": [
    "## Programa principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac019ce5-4388-4667-89c6-99f5f75bd8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí debes insertar el programa principal que llama al resto de funciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9514dfa1-6f76-4b14-86e3-dce3ae6ded91",
   "metadata": {},
   "source": [
    "## Capturas de pantalla\n",
    "\n",
    "A partir de aquí tienes que insertar las capturas de pantalla correspondientes a cada punto. Las capturas de pantalla corresponderán a la interfaz gráfica de la base de datos correspondiente y se debe mostrar que los datos se han cargado correctamente. Los apartados que no tengan la captura de pantalla correspondiente **se considerarán no realizados**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45932542-a53f-4672-9680-3a735fd9d580",
   "metadata": {},
   "source": [
    "### Captura de InfluxDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fad5c6-b160-4f5a-8d8a-832bdb21c0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ea0c97a-6181-4257-972f-52d2915a1e77",
   "metadata": {},
   "source": [
    "### Captura de estadísticas agregadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee9dd4a-a7af-4e75-9274-3cc13ec7ae55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab967f49-1886-4bee-953b-61a20f4bcb5b",
   "metadata": {},
   "source": [
    "### Captura de ranking de puntos calientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215c849-1568-4e2a-adaa-8534bac7d105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a06cfcb-7677-425d-bc63-7e85042d8c62",
   "metadata": {},
   "source": [
    "### Captura de seguimiento de flota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97575442-6c6f-4309-bd64-ddb670c2ff8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "589bae22-0d26-4f38-9bcb-db3eec184f8c",
   "metadata": {},
   "source": [
    "### Captura de contadores globales atómicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3747246-e0af-47c6-9509-eaf2d0f20527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a301380-6041-411e-9289-f15b97ceac37",
   "metadata": {},
   "source": [
    "### Captura de cola de anomalías críticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79aa60eb-3657-43ed-8bd0-eab4fb72f779",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
